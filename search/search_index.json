{"config":{"lang":["fr"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Projections avec les LLMs","text":"<p>Th\u00e8mes: Science de donn\u00e9es, G\u00e9nie logiciel, LLM Superviseur: Louis Edouard Lafontant Collaborateurs: ...</p>"},{"location":"#equipe","title":"\u00c9quipe","text":"<p>Esteban Maries 20235999</p>"},{"location":"#description-du-projet","title":"Description du projet","text":""},{"location":"#contexte","title":"Contexte","text":"<p>Le domaine d'application de ce projet comprend l'Ing\u00e9nierie Logicielle ainsi que du Traitement Automatique du Langage Naturel (NLP), avec une emphase sur l'Analyse du Code Source et la Compr\u00e9hension de Programme. Alors que les syst\u00e8mes logiciels gagnent en complexit\u00e9, l'int\u00e9gration des Grands Mod\u00e8les de Langage (LLMs) dans les flux de travail d'analyse de code devient essentielle, presque obligatoire pour am\u00e9liorer l'efficacit\u00e9, la pr\u00e9cision et l'automatisation du code.</p> <p>Cependant, les d\u00e9veloppeurs passent beaucoup de temps \u00e0 chercher et dig\u00e9rer les informations du code source qui permettent d\u2019y contribuer ou de le comprendre, surtout lorsqu'ils travaillent avec du code peu familier ou des environnements complexes.</p> <p>Une approche propos\u00e9e pour att\u00e9nuer ce probl\u00e8me est le concept de projections de code. Ces projections permettent de visualiser le m\u00eame code source de multiples fa\u00e7ons dynamiques. Au lieu d'une structure rigide, le programmeur peut choisir une structure qu'il juge la plus pertinente pour sa t\u00e2che actuelle.</p> <p>Ce projet s'inscrit comme une extension possible du projet Gentleman visant \u00e0 rendre la mod\u00e9lisation plus accessible aux experts du domaine et aux praticiens.</p>"},{"location":"#problematique-ou-motivations","title":"Probl\u00e9matique ou motivations","text":""},{"location":"#problematique","title":"Probl\u00e9matique","text":"<p>La probl\u00e9matique principale que ce projet cherche \u00e0 r\u00e9soudre est de combler le foss\u00e9 qui existe entre la compr\u00e9hension profonde du code, fournie par les LLMs, et la pr\u00e9sentation structur\u00e9e et dynamique du code source, donn\u00e9 par Gentleman. Comment utiliser un LLM pour fournir les fragments de code pertinents et les pr\u00e9senter aux d\u00e9veloppeurs sous forme de projections compr\u00e9hensible ?</p>"},{"location":"#motivations","title":"Motivations","text":"<ol> <li> <p>Les d\u00e9veloppeurs passent un temps consid\u00e9rable \u00e0 comprendre le comportement et la logique du code pour faciliter son \u00e9dition et maintenance. En fournissant des projections cibl\u00e9es, l'outil pourrait r\u00e9duire consid\u00e9rablement la quantit\u00e9 de code \u00e0 parcourir par le d\u00e9veloppeur.</p> </li> <li> <p>Bien que les LLMs excellent \u00e0 expliquer le code en langage naturel, les requ\u00eates ouvertes peuvent encore n\u00e9cessiter un prompt engineering difficile, en particulier pour les novices. L'int\u00e9gration d'un m\u00e9canisme de projection permet de mat\u00e9rialiser l'analyse du LLM dans des vues concr\u00e8tes et interactives, offrant une assistance visuelle du code au d\u00e9veloppeur.</p> </li> </ol>"},{"location":"#proposition-et-objectifs","title":"Proposition et objectifs","text":""},{"location":"#proposition-solution","title":"Proposition solution","text":"<ol> <li> <p>Lecture et Contexte : Le syst\u00e8me lira le code source.</p> </li> <li> <p>Analyse par LLM : Un LLM analysera le code pour comprendre sa structure et sa fonctionnalit\u00e9, et pour identifier les fragments de code qui correspondent \u00e0 des pr\u00e9occupations s\u00e9mantiques implicites. Son importance, r\u00f4le dans le code</p> </li> <li> <p>Sortie des Projections : Les sorties du LLM seront donnes en deux fichiers un qui est les projections et l\u2019autre les concepts qui d\u00e9finiront le code source pour Gentleman.</p> </li> </ol>"},{"location":"#objectifs-concrets","title":"Objectifs Concrets","text":"<ol> <li> <p>D\u00e9finir et impl\u00e9menter une m\u00e9thodologie pour contextualiser le code source et formuler les requ\u00eates aupr\u00e8s du LLM afin que ce dernier puisse identifier les diff\u00e9rents fragments de code pertinents, ainsi que leurs liens dans le code source.</p> </li> <li> <p>Manier le LLM afin qu\u2019il renvoie dans le format attendu par Gentleman, c\u2019est \u00e0 dire un fichier projection et un fichier concept. Qui permettront \u00e0 Gentleman de modeler le code source donn\u00e9 par le d\u00e9veloppeur.</p> </li> </ol>"},{"location":"#echeancier","title":"\u00c9ch\u00e9ancier","text":"<p>Info</p> <p>Le suivi complet est disponible dans la page Suivi de projet.</p> Jalon (Milestone) Date pr\u00e9vue Livrable Statut Ouverture de projet 16 septembre Choix de projet \u2705 Termin\u00e9 Definir le projet 28 septembre Description du projet remis \u2705 Termin\u00e9 Definition de fonctions 5 octobre Document sur les fonctions \u2705 Termin\u00e9 Choix LLM 5 octobre LLM \u2705 Termin\u00e9 Essai de la LLM 8 septembre Resultats \u2705 Termin\u00e9 Ajout d'un template json du resultat attendu 30 septembre Fichier JSON \u2705 Termin\u00e9 Ajout de profondeur d'exploration/d\u00e9finition du LLM 6 octobre Code \u2705 Termin\u00e9 Correction des d\u00e9fauts dans l'architecture 12 octobre Code \u2705 Termin\u00e9 Peaufinage des prompts et du resultats de sortie 20 octobre Code et r\u00e9sultats \u2705 Termin\u00e9 Reconstruction de la structure de requete 11 novembre Code \u2705 Termin\u00e9 Ajout d'informations extraites par fonctions 20 novembre Code et r\u00e9sultats \u2705 Termin\u00e9 Creation de l'API et last minute code check 30 novembre Plan + R\u00e9sultats interm\u00e9diaires \u2705 Termin\u00e9 Ecriture du suivi et du rapport 1 d\u00e9cembre Analyse des r\u00e9sultats + Discussion \ud83d\udd04 En cours Pr\u00e9sentation 11 d\u00e9cembre Pr\u00e9sentation \u23f3 \u00c0 venir Rapport 19 d\u00e9cembre Rapport \u23f3 \u00c0 venir"},{"location":"analysis/","title":"\u00c9tudes pr\u00e9liminaires","text":""},{"location":"analysis/#analyse-du-probleme","title":"Analyse du probl\u00e8me","text":"<ul> <li>Il faut pouvoir, \u00e0 partir d'un fichier ou d'une liste de fichiers de code, d\u00e9finir les fonctions dans chaque fichier, d\u00e9terminer leurs caract\u00e9ristiques, et toute information qui peut \u00eatre utile et \u00eatre utilis\u00e9e dans Gentleman. Ceci en faisant appel \u00e0 une LLM pour d\u00e9finir certaines parties des fonctions qui ne sont pas explicitement \u00e9crites.</li> </ul>"},{"location":"analysis/#exigences","title":"Exigences","text":"<ul> <li>La LLM doit pouvoir d\u00e9finir d'une fonction, les types des param\u00e8tres, le type du retour, sa cat\u00e9gorie, les appels qu'elle fait et qui l'appelle, sa description, et enfin des tags visant \u00e0 d\u00e9finir la fonction en mots-cl\u00e9s pour un r\u00e9f\u00e9rencement.</li> </ul>"},{"location":"analysis/#recherche-de-solutions","title":"Recherche de solutions","text":"<ul> <li>Une premi\u00e8re solution a \u00e9t\u00e9 d'utiliser tout le fichier et de demander au LLM de d\u00e9finir toutes les fonctions d'un coup, mais ceci s'est av\u00e9r\u00e9 trop compliqu\u00e9 pour l'intelligence du mod\u00e8le utilis\u00e9, et instable entre chaque run, rendant son utilisation dure puisque les r\u00e9ponses du mod\u00e8le ne sont pas passable.</li> <li>Une autre solution, qui a \u00e9t\u00e9 celle adopt\u00e9e par nous, est de d\u00e9finir les fonctions une \u00e0 la fois, et de faire la m\u00eame chose lors de l'extraction d'information par le mod\u00e8le. Ceci permet de valider les r\u00e9ponses \u00e0 chaque \u00e9tape et donne un plus grand contr\u00f4le sur les r\u00e9ponses possibles du mod\u00e8le.</li> </ul>"},{"location":"analysis/#methodologie","title":"M\u00e9thodologie","text":"<ul> <li>Utilisation d'une approche it\u00e9rative, o\u00f9 on construit le code, et l'on modifie au fur et \u00e0 mesure les diff\u00e9rents aspects jusqu'\u00e0 obtenir le r\u00e9sultat voulu. Cette approche a sembl\u00e8rent march\u00e9 m\u00eame si, pour les prompts lors de la premi\u00e8re solution, cette approche a \u00e9t\u00e9 peu fructueuse dans les r\u00e9sultats. Cependant, une fois l'autre solution trouv\u00e9e, le rendu fut presque complet, hormis le manque de profondeur/abstraction possible, qui n'ont pas \u00e9t\u00e9 essay\u00e9s par manque de temps.</li> </ul>"},{"location":"conception/","title":"Conception","text":""},{"location":"conception/#architecture","title":"Architecture","text":"<ul> <li>On a une classe Gentleman LLM qui contient la partie qui va analyser les fichiers et d\u00e9finir les fonctions.</li> <li>Un fichier gentleman request qui est l'API gentleman LLM qui recevra les fichiers de l'utilisateur, ainsi que les requ\u00eates pour analyser ces fichiers.</li> </ul>"},{"location":"conception/#choix-technologiques","title":"Choix technologiques","text":"<ul> <li>On utilise Python et le module AST pour parcourir le contenu des fichiers et pour le parser. Ceci a \u00e9t\u00e9 fait pour la simplicit\u00e9 d'usage qui vient avec Python pour faire des requ\u00eates vers des LLMs. De plus, l'id\u00e9e ici \u00e9tait plut\u00f4t de montrer la possibilit\u00e9 d'utiliser des LLMs pour Gentleman, on cherchait simplement \u00e0 faire un fichier dans le format d'un concept Gentleman.</li> </ul>"},{"location":"conception/#prototype","title":"Prototype","text":"<ul> <li>Voici un exemple d'appel de l'API et un r\u00e9sultat envisageable:</li> </ul> <p>Upload du fichier vers Gentleman</p> Python <pre><code>resp = requests.post(\n    \"http://127.0.0.1:8000/upload\",\n    json={\n        \"filename\": \"list_files.py\",\n        \"content\": '''def list_files(directory: str) -&gt; list[str]:\\n    \\\"\\\"\\\"Lists all files in a given directory.\\n\\n    Args:\\n        directory (str): The name of the directory\\n\\n    Returns:\\n        list[str]: Lists of all file names in a directory.\\n    \\\"\\\"\\\"\\n\\n    try:\\n        return [\\n            f\\n            for f in os.listdir(directory)\\n            if os.path.isfile(os.path.join(directory, f))\\n        ]\\n    except FileNotFoundError:\\n        print(f\\\"Directory not found: {directory}\\\")\\n        return []\\n    except Exception as e:\\n        print(f\\\"Error listing files in {directory}: {e}\\\")\\n        return []''',\n    },\n)\n</code></pre> <p>Analyse du fichier par GentlemanLLM</p> Python <pre><code>resp = requests.post(\n    \"http://127.0.0.1:8000/analyze\",\n    json={\n        \"filepath\": \"list_files.py\",\n        \"model\": \"meta-llama/Llama-3.1-8B-Instruct\",\n        \"hf_token\": hf_token,\n    },\n)\n</code></pre> <p>Resultat possible</p> JSON <pre><code>[\n    {\n        \"file\": \"temp.py\"\n    },\n    {\n        \"name\": \"list_files\",\n        \"parameters\": [\n            [\n                \"directory\",\n                \"str\"\n            ]\n        ],\n        \"source\": \"def list_files(directory: str) -&gt; list[str]:\\n    \\\"\\\"\\\"Lists all files in a given directory.\\n\\n    Args:\\n        directory (str): The name of the directory\\n\\n    Returns:\\n        list[str]: Lists of all file names in a directory.\\n    \\\"\\\"\\\"\\n\\n    try:\\n        return [\\n            f\\n            for f in os.listdir(directory)\\n            if os.path.isfile(os.path.join(directory, f))\\n        ]\\n    except FileNotFoundError:\\n        print(f\\\"Directory not found: {directory}\\\")\\n        return []\\n    except Exception as e:\\n        print(f\\\"Error listing files in {directory}: {e}\\\")\\n        return []\",\n        \"start_line\": 3,\n        \"end_line\": 24,\n        \"called_by\": [],\n        \"calls\": [],\n        \"description\": \"\\\"Lists all files in a given directory, handling exceptions.\\\"\",\n        \"tags\": [\n            \"file_management\",\n            \"directory_scanning\",\n            \"os_interaction\",\n            \"file_listing\",\n            \"filesystem\"\n        ],\n        \"category\": \"ExternalInteraction\",\n        \"return\": [\n            \"[f for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\",\n            \"list\"\n        ]\n    }\n]\n</code></pre>"},{"location":"evaluation/","title":"\u00c9valuation","text":""},{"location":"evaluation/#analyse-des-resultats","title":"Analyse des r\u00e9sultats","text":"<ul> <li>A travers la session, on a cr\u00e9\u00e9 plusieurs fichiers r\u00e9sultats. Les premi\u00e8res it\u00e9rations furent le r\u00e9sultat brut de la LLM qui \u00e9tait instable, et changeante constamment entre les diff\u00e9rents runs.</li> <li>On a ensuite eu les premiers r\u00e9sultats sous forme JSON, essayant de mimiques le fichier concept attendu. Mais d\u00fb \u00e0 notre LLM, les r\u00e9sultats furent peu convaincants et manquaient de ma\u00eetrise, les champs n'\u00e9taient pas souvent les m\u00eames, des champs ont \u00e9t\u00e9 cr\u00e9\u00e9s par le mod\u00e8le, d'autres enlev\u00e9s, ce n'\u00e9tait pas viable.</li> <li>Enfin, on a les derniers r\u00e9sultats JSON, qui sont portables pour Gentleman et cr\u00e9\u00e9s par it\u00e9ration. Ces r\u00e9sultats sont tr\u00e8s convaincants et trouvent une possibilit\u00e9, et un futur pour Gentleman LLM. Bien que les informations extraites soient minimes, elles s'obtiennent relativement rapidement et de mani\u00e8re assez stable. La LLM semble mieux comprendre ce que l'on lui demande et arrive \u00e0 mieux d\u00e9finir les fonctions, de mani\u00e8re plus constante.</li> </ul>"},{"location":"references/","title":"R\u00e9f\u00e9rences","text":"<ul> <li>Using an LLM to Help With Code Understanding</li> <li>Toward Intelligent Generation of Tailored Graphical Concrete Syntax</li> <li>Large Language Models (LLMs) for Source Code Analysis: applications, models and datasets</li> <li>Leveraging Program Comprehension with Concern-oriented Source Code Projections</li> <li>Inclure des liens vers des sites web, outils ou technologies utilis\u00e9s.</li> </ul>"},{"location":"resources/","title":"Analyse statique avec <code>ast</code> dans Gentleman-LLM","text":"<p>Gentleman-LLM utilise le module Python <code>ast</code> pour analyser automatiquement le contenu d\u2019un fichier Python.  L\u2019objectif est d\u2019extraire des informations structur\u00e9es pour guider la LLM dans l\u2019analyse des fonctions.</p>"},{"location":"resources/#quest-ce-que-le-module-ast","title":"Qu\u2019est-ce que le module <code>ast</code> ?","text":"<p><code>ast</code> signifie Abstract Syntax Tree, Python peut transformer du code en un arbre de n\u0153uds, chaque n\u0153ud repr\u00e9sentant un \u00e9l\u00e9ment du langage :</p> <ul> <li><code>FunctionDef</code> \u2192 une fonction  </li> <li><code>Return</code> \u2192 une instruction return  </li> <li><code>Call</code> \u2192 un appel de fonction  </li> <li><code>Import</code> \u2192 un import  </li> <li><code>Name</code> \u2192 une variable  </li> <li>etc.</li> </ul> <p>Cela permet d\u2019analyser du code de fa\u00e7on s\u00fbre, et fiable.</p>"},{"location":"resources/#ast-utilise-dans-gentleman-llm","title":"AST utilis\u00e9 dans Gentleman-LLM","text":"<p>Voici l\u2019ordre dans lequel on r\u00e9cup\u00e8re les informations :</p> <pre><code>flowchart TD\n    A[Lire le fichier Python] --&gt; B[ast.parse \u2192 Arbre syntaxique]\n    B --&gt; C[Extraire les imports]\n    B --&gt; D[Extraire les fonctions]\n    D --&gt; E[Param\u00e8tres]\n    D --&gt; F[Code source]\n    D --&gt; G[Type de return]\n    B --&gt; H[Appels entre fonctions]\n    H --&gt; I[Relations calls / called_by]\n    I --&gt; J[R\u00e9sultat final envoy\u00e9 au LLM]\n</code></pre>"},{"location":"resources/#interaction-llm-dans-gentleman-llm","title":"Interaction LLM dans Gentleman-LLM","text":"<p>Gentleman-LLM ne se contente pas d\u2019analyser statiquement le code avec <code>ast</code>. Une fois que les informations sont extraites, le syst\u00e8me interroge une LLM pour :</p> <ul> <li>d\u00e9duire les types de param\u00e8tres  </li> <li>inf\u00e9rer des tags  </li> <li>g\u00e9n\u00e9rer une description concise  </li> <li>d\u00e9terminer la cat\u00e9gorie de fonction  </li> <li>ajuster le type de retour si n\u00e9cessaire  </li> </ul> <p>Le module responsable de cette \u00e9tape est <code>GentlemanLLM</code>, qui communique avec un mod\u00e8le OpenAI via <code>HuggingFace</code>.</p>"},{"location":"results/","title":"R\u00e9sultats","text":""},{"location":"results/#fonctionnalites","title":"Fonctionnalit\u00e9s","text":"<p>Gentleman-LLM est un syst\u00e8me d\u2019analyse automatis\u00e9e du code Python bas\u00e9 sur :</p> <ol> <li> <p>Analyse statique du code avec <code>ast</code> </p> <ul> <li>Extraction de toutes les fonctions d\u2019un fichier.  </li> <li>R\u00e9cup\u00e9ration des param\u00e8tres, lignes de d\u00e9but/fin, corps exact du code.</li> <li>D\u00e9tection des imports.</li> <li>Analyse des appels internes entre fonctions.  </li> <li>Inf\u00e9rence partielle du type de retour.</li> </ul> </li> <li> <p>Enrichissement s\u00e9mantique via une LLM </p> <ul> <li>D\u00e9duction automatique des types de param\u00e8tres.  </li> <li>G\u00e9n\u00e9ration d\u2019une description concise du r\u00f4le de chaque fonction.  </li> <li>Cr\u00e9ation de tags repr\u00e9sentatifs.  </li> <li>D\u00e9termination de la cat\u00e9gorie de la fonction.  </li> <li>Correction/validation du type de retour si n\u00e9cessaire.</li> </ul> </li> <li> <p>Validation stricte des r\u00e9ponses du mod\u00e8le </p> <ul> <li>V\u00e9rification du format des types.  </li> <li>Validation des cat\u00e9gories.  </li> <li>Gestion des erreurs et tentative automatique jusqu\u2019\u00e0 stabilit\u00e9.  </li> <li>Nettoyage et normalisation des sorties du mod\u00e8le.</li> </ul> </li> <li> <p>Structure finale des donn\u00e9es     Pour chaque fichier Python analys\u00e9, Gentleman-LLM g\u00e9n\u00e8re un objet JSON contenant :</p> <ul> <li>le nom du fichier  </li> <li>la liste compl\u00e8te des fonctions  </li> <li>pour chaque fonction :  <ul> <li>nom  </li> <li>param\u00e8tres et leurs types  </li> <li>description g\u00e9n\u00e9r\u00e9e  </li> <li>tags  </li> <li>cat\u00e9gorie  </li> <li>type de retour  </li> <li>relations (calls / called_by)  </li> <li>code source exact  </li> </ul> </li> </ul> </li> <li> <p>Export automatique </p> <ul> <li>Renvoie le r\u00e9sultat sous forme de JSON dans <code>documents/results/...</code> </li> <li>Versionnement automatique (filename_1.json, filename_2.json, etc.)</li> </ul> </li> </ol>"},{"location":"suivi/","title":"Suivi de projet","text":""},{"location":"suivi/#semaine-1","title":"Semaine 1","text":"<p>Notes</p> <ul> <li>Lecture de la documentation sur les LLMs, leur usage sur du code ainsi que celle sur Gentleman.</li> <li>\u00c9criture de la description du projet</li> </ul> <p>Prochaines \u00e9tapes</p> <ul> <li>Choisir une LLM pour le projet</li> <li>D\u00e9finition des fonctions dans le code</li> <li>Essayer la LLM</li> </ul>"},{"location":"suivi/#semaine-3","title":"Semaine 3","text":"<p>Notes</p> <ul> <li>D\u00e9finition de diff\u00e9rents types de fonctions pour la LLM</li> <li>Choix et essai de la LLM</li> </ul> <p>Probl\u00e8me possible</p> <ul> <li>Il faut pouvoir avoir assez de cr\u00e9dits pour faire des requ\u00eates aupr\u00e8s du LLM.</li> </ul> <p>Avance</p> <ul> <li>Premier essai plut\u00f4t bon.</li> </ul>"},{"location":"suivi/#semaine-5","title":"Semaine 5","text":"<p>Notes</p> <ul> <li>Besoin de d\u00e9finir de mani\u00e8re plus s\u00e9v\u00e8re les instructions</li> <li>Besoin d'un template pour le <code>concept.json</code></li> </ul> <p>Probl\u00e8mes</p> <ul> <li>Le LLM ne suit pas toujours les instructions, cr\u00e9e des <code>tags</code> qui n'existent pas ainsi que fait des actions explicitement interdites.</li> </ul>"},{"location":"suivi/#semaine-7","title":"Semaine 7","text":"<p>Notes</p> <ul> <li>Ajout du template <code>concept.json</code> au projet</li> <li>Ajout de r\u00e8gles, \u00e9tapes de r\u00e9flexion, d\u00e9finition de concepts Gentleman, d'un persona, ainsi que 3 niveaux de profondeur dans l'analyse.</li> <li>Les trois niveaux sont :<ul> <li>Niveau 0 : contexte level C4 model (type de fonction, nature, param\u00e8tres, return types)</li> <li>Niveau 1: container level C4 model (Niv 0 + relations avec d'autres fonctions)</li> <li>Niveau 2 : abstraction semantic et relationnel (Niv 1 + contraintes dans la fonction).</li> </ul> </li> <li>Changement de moonshotai \u00e0 meta-llama d\u00fb aux co\u00fbts des requ\u00eates.</li> <li>Division en deux entre les types de fonctions classiques et celles plus pr\u00e9cises.</li> <li>Le LLM renvoie des classifications plus similaires \u00e0 chaque run</li> </ul> <p>Probl\u00e8mes</p> <ul> <li>La structure du JSON n'est pas garantie, et le LLM renvoie parfois du texte en plus.</li> </ul>"},{"location":"suivi/#semaine-9","title":"Semaine 9","text":"<p>Notes</p> <ul> <li>S\u00e9paration du code, cr\u00e9ation du fichier <code>util.py</code> qui contiendra les fonctions utilitaires utilis\u00e9es par le LLM et nous-m\u00eames.</li> <li>Enrichissement des prompts pour essayer de faire mieux comprendre le LLM.</li> <li>Ajout d'une note de hi\u00e9rarchie des prompts pour la LLM.</li> <li>Cr\u00e9ation d'une fonction principale pour extraire les fonctions d'un fichier.</li> <li>Modification des types de fonctions pour simplifier la t\u00e2che au LLM, et clarification des points n\u00e9cessaires, optionnels, ou qu'elle ne doit pas avoir.</li> <li>Ajout d'un champ <code>reasoning</code> pour que l'on comprenne le choix du LLM.</li> <li>Simplification des prompts dans l'id\u00e9e de ne pas surcharger la LLM.</li> </ul> <p>Probl\u00e8mes</p> <ul> <li>Il y a quelques r\u00e9sultats, mais le LLM semble vraiment avoir du mal \u00e0 output le JSON dans le bon format avec les bonnes valeurs.</li> <li>Le LLM renvoie parfois des fichiers sans JSON, et dans un format qui n'est pas stable (change entre chaque run).</li> </ul>"},{"location":"suivi/#semaine-11","title":"Semaine 11","text":"<p>Notes</p> <ul> <li>Changement de fichier retour, JSON -&gt; YAML, dans l'espoir que le LLm soit plus stable dans ces r\u00e9ponses. Changement aussi du fichier contenant les types de fonctions de JSON vers du texte dans local.py.</li> <li>Essaie de faire en sorte que la LLM renvoie seulement les types des param\u00e8tres de la fonction, et de r\u00e9ussir \u00e0 valider sa r\u00e9ponse.</li> <li>Changement de la structure de Gentleman LLM, d\u00e9sormais les fonctions auront leurs parties individuelles d\u00e9crites par la LLM. La LLM va donc d\u00e9finir les types des param\u00e8tres, la description de chaque fonction, les tags, un \u00e0 un, \u00e0 la place de toutes les fonctions en un coup.</li> </ul> <p>Probl\u00e8mes</p> <ul> <li>R\u00e9ponse du LLM n'est pas stable, pas un format unique et entrave grandement la possibilit\u00e9 d'utiliser ses r\u00e9ponses qui ne sont pas portables, manquant de structure.</li> <li>Abandon de la premi\u00e8re id\u00e9e pour Gentleman LLM, les requ\u00eates ne se feront pas sur tout le fichier d'un coup mais sur chaque fonction afin de nous permettre de  pouvoir parser les r\u00e9ponses du mod\u00e8le.</li> </ul>"},{"location":"suivi/#semaine-13","title":"Semaine 13","text":"<p>Notes</p> <ul> <li>Cr\u00e9ation de la base de la nouvelle structure du code, ajout de la possibilit\u00e9 de r\u00e9essayer des kerries, si un output n'est pas valide.</li> <li>Ajout de la d\u00e9finition du type de retour et de la cat\u00e9gorie de la fonction.</li> <li>Cr\u00e9ation de logging et erreur handling pour chaque partie de la d\u00e9finition des fonctions.</li> <li>Cr\u00e9ation d'une fonction d\u00e9di\u00e9e \u00e0 g\u00e9n\u00e9rer les types valides Python que l'on comparera aux r\u00e9ponses du LLM.</li> <li>Ajout des imports du fichier dans lequel se situe la fonction afin de les utiliser comme type pour comparer aux r\u00e9ponses du LLM.</li> <li>Cr\u00e9ation de la classe Gentleman LLM qui contiendra le code principal dans l'analyse de fichiers et leurs fonctions.</li> <li>Cr\u00e9ation des gentleman requests, uploads et analyses qui forment l'API de Gentleman LLM.</li> </ul> <p>Probl\u00e8mes</p> <ul> <li>La d\u00e9finition des types de retour des fonctions n'est pas correcte \u00e0 100%, une erreur d'attribution de valeur ou de type peut avoir lieu lorsque la fonction contient plusieurs retours, ceci est d\u00fb \u00e0 la fa\u00e7on dont AST parcours les fichiers Python.</li> </ul>"}]}